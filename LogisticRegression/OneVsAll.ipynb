{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass the percentage of data you need for test like 20 % \n",
    "def train_split(df,test_per):\n",
    "    indices=df.index.tolist()\n",
    "    test_size=round(len(df)*(test_per/100))\n",
    "    random.seed(0)\n",
    "    test_indices=random.sample(population=indices,k=test_size)\n",
    "    valid_df=df.loc[test_indices]\n",
    "    train_df=df.drop(test_indices)\n",
    "    return train_df,valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_hypothesis(x,B):\n",
    "    #hypothesis h(x) = 1/(1+ pow(e,-(B.T*x)))   #hypothesis=X.T*B as X=data.T\n",
    "    t=x.dot(B)\n",
    "    exp_part=math.exp(float(-t))\n",
    "    hypothesis=1/(1+exp_part)\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X,Y,B):\n",
    "    #Cost function J=(-1/m)*(sum((yi*log(hypothesis(xi)))+((1-yi)*log(1-hypothesis(xi))))\n",
    "    m = len(Y)\n",
    "    J=0\n",
    "    for i in range(0,m):\n",
    "        x=X[i]\n",
    "        y=Y[i]\n",
    "        h=cal_hypothesis(x,B)\n",
    "        J-=((y*np.log(h))+((1-y)*np.log(1-h)))\n",
    "    J/=m\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,Y,B,alpha,prev_cost,no_iters):\n",
    "    #formula: Bj=Bj-alpha*((hypothesis(x)-y)*xj)\n",
    "    if(no_iters==0):\n",
    "        return B,prev_cost\n",
    "    m=len(Y)\n",
    "    for j in range(0,len(B)):\n",
    "        sum=0\n",
    "        gradient=1\n",
    "        for i in range(0,m):\n",
    "            x=X[i]\n",
    "            y=Y[i]\n",
    "            hypothesis=cal_hypothesis(x,B)\n",
    "            sum+=(hypothesis-y)*x[j]\n",
    "        gradient=sum\n",
    "        #update B\n",
    "        B[j]-=alpha*gradient\n",
    "    cost=cost_function(X,Y,B)\n",
    "    #print(prev_cost,cost)\n",
    "    if(abs(prev_cost - cost) <  0.000001):\n",
    "        return B,cost\n",
    "    else:\n",
    "        B,cost=gradient_descent(X,Y,B,alpha,cost,no_iters-1)\n",
    "    return B,cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(x,B):\n",
    "    prob=cal_hypothesis(x,B)\n",
    "    #print(\"Predicted prob-\",prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ans(x,B,threshold):\n",
    "    prob=predict_prob(x,B)\n",
    "    if(prob >= threshold):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_all(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat(X,Y,B,threshold):\n",
    "    true_val=1\n",
    "    false_val=0\n",
    "    total=len(X)\n",
    "    actual_list=[]\n",
    "    predicted_list=[]\n",
    "    TN=0\n",
    "    TP=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    for i in range(0,total):\n",
    "        actual = Y[i]\n",
    "        actual_list.append(actual)\n",
    "        predicted = predict_ans(X[i],B,threshold)\n",
    "        predicted_list.append(predicted)\n",
    "        if( actual > threshold):\n",
    "            actual=1\n",
    "        else:\n",
    "            actual=0\n",
    "        if(actual==false_val and predicted==false_val):\n",
    "            TN+=1\n",
    "        if(actual==true_val and predicted==false_val):\n",
    "            FN+=1\n",
    "        if(actual==false_val and predicted==true_val):\n",
    "            FP+=1\n",
    "        if(actual==true_val and predicted==true_val):\n",
    "            TP+=1\n",
    "    #measures=[accuracy,misclassification,precision,recall,f1score]\n",
    "    measures=[]\n",
    "    accuracy=(TN+TP)/total\n",
    "    measures.append(accuracy)\n",
    "    misclassification=(FN+FP)/total\n",
    "    measures.append(misclassification)\n",
    "    if( TP+FP > 0):\n",
    "        precision=TP/(TP+FP)\n",
    "    else:\n",
    "        precision=0\n",
    "    measures.append(precision)\n",
    "    if( TP+FN > 0):\n",
    "        recall=TP/(TP+FN)\n",
    "    else:\n",
    "        recall=0\n",
    "    measures.append(recall)\n",
    "    if( precision !=0 and recall !=0):\n",
    "        f1score=2/((1/precision)+(1/recall))\n",
    "    else:\n",
    "        f1score=0\n",
    "    measures.append(f1score)\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(Y,predictions):\n",
    "    total=len(Y)\n",
    "    correct=0\n",
    "    for i in range(0,total):\n",
    "        actual = Y[i]\n",
    "        predicted = predictions[i]\n",
    "        if(actual == predicted):\n",
    "            correct+=1\n",
    "    accuracy=correct/total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(actual_list,predicted_list,title):\n",
    "    x_labels=[]\n",
    "    for i in range(len(actual_list)):\n",
    "        x_labels.append(i)\n",
    "        \n",
    "    area=np.pi\n",
    "    plt.xlabel(\"Students\")\n",
    "    plt.ylabel(\"Chance of Admit\")\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.scatter(x_labels, predicted_list, s=area, c='blue', alpha=0.5)\n",
    "    plt.scatter(x_labels, actual_list, s=area, c='red', alpha=0.5)\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X(data.T),Y arrays from df\n",
    "def to_arrays(df,label):\n",
    "    no_rows,no_cols=df.shape\n",
    "    header=list(df.columns)\n",
    "    data_array=np.ones((no_cols-1,no_rows))\n",
    "    X0=np.ones(no_rows)                      #X0=1\n",
    "    for i in range(0,no_cols-1):             #Removing first col-serial no , last col-label\n",
    "        if(i == 0):\n",
    "            data_array[i]=X0\n",
    "        else:\n",
    "            x=df[header[i]].values\n",
    "            #data_array[i]=x\n",
    "            data_array[i]=mean_normalize(x)\n",
    "    X=data_array.T        #Doing Transpose\n",
    "\n",
    "    #Y (output) array\n",
    "    Y=np.array(df[label].values)  \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalize(x):\n",
    "    x_new=np.ones(len(x))\n",
    "    mean=np.mean(x)\n",
    "    std=np.nanstd(x)\n",
    "    for i in range(0,len(x)):\n",
    "        x_new[i]=(x[i]-mean)/std\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file=\"wine-quality/data.csv\"\n",
    "    label='quality'\n",
    "    no_coefficients=11  #[B0,B1,B2,...,B10] also 10 columns but X0=1    \n",
    "        \n",
    "    df=pd.read_csv(file)\n",
    "    \n",
    "    #no_classes=df[label].nunique()  \n",
    "    no_classes=11    #Multiclass (0-10)\n",
    "   \n",
    "    train_df,valid_df=train_split(df,20)\n",
    "    \n",
    "    #Training\n",
    "    train_X,train_Y=to_arrays(train_df,label)\n",
    "    \n",
    "    alpha = 0.003\n",
    "    no_iters=1000\n",
    "    \n",
    "    ##One VS All\n",
    "    \n",
    "    print(\"One Vs All\")\n",
    "    #B(beta) array of coefficients\n",
    "    #Initialized to zero\n",
    "    B = np.zeros(shape=(no_classes,no_coefficients))\n",
    "    \n",
    "    for c in range(0, no_classes):\n",
    "        y_new=np.zeros(len(train_Y))\n",
    "        for i in range(0,len(train_Y)):\n",
    "            if(train_Y[i]==c):\n",
    "                y_new[i]=1\n",
    "            else:\n",
    "                y_new[i]=0\n",
    "\n",
    "        init_J=cost_function(train_X,y_new,B[c])\n",
    "        print(\"Initial_cost- \",init_J)\n",
    "\n",
    "        B[c],final_J=gradient_descent(train_X,y_new,B[c],alpha,init_J,no_iters)\n",
    "        print(\"Final_cost- \",final_J)\n",
    "        #print(B[c])\n",
    "\n",
    "    #Prediction\n",
    "    valid_X,valid_Y=to_arrays(valid_df,label)\n",
    "\n",
    "    classProbs=sigmoid_all(valid_X @ B.T)\n",
    "    #print(classProbs)    \n",
    "\n",
    "    prediction=np.zeros(len(valid_df))\n",
    "    \n",
    "    for i in range(len(classProbs)):\n",
    "        prediction[i]=np.argmax(classProbs[i])\n",
    "     \n",
    "    acc=cal_accuracy(valid_Y,prediction)\n",
    "    print(\"Accuracy-\",acc)\n",
    "    \n",
    "    \n",
    "#     ##One VS One\n",
    "    \n",
    "#     print(\"One Vs One\")\n",
    "#     #B(beta) array of coefficients\n",
    "#     #Initialized to zero\n",
    "#     B = np.zeros(shape=(no_classes,no_coefficients))\n",
    "    \n",
    "#     for i in range(0, no_classes-1):\n",
    "#         for j in range(0,)\n",
    "#         y_new=np.zeros(len(train_Y))\n",
    "#         for i in range(0,len(train_Y)):\n",
    "#             if(train_Y[i]==c):\n",
    "#                 y_new[i]=1\n",
    "#             else:\n",
    "#                 y_new[i]=0\n",
    "\n",
    "#         init_J=cost_function(train_X,y_new,B[c])\n",
    "#         print(\"Initial_cost- \",init_J)\n",
    "\n",
    "#         B[c],final_J=gradient_descent(train_X,y_new,B[c],alpha,init_J,no_iters)\n",
    "#         print(\"Final_cost- \",final_J)\n",
    "#         #print(B[c])\n",
    "\n",
    "#     #Prediction\n",
    "#     valid_X,valid_Y=to_arrays(valid_df,label)\n",
    "\n",
    "#     classProbs=sigmoid_all(valid_X @ B.T)\n",
    "#     #print(classProbs)    \n",
    "\n",
    "#     prediction=np.zeros(len(valid_df))\n",
    "    \n",
    "#     for i in range(len(classProbs)):\n",
    "#         prediction[i]=np.argmax(classProbs[i])\n",
    "     \n",
    "#     acc=cal_accuracy(valid_Y,prediction)\n",
    "#     print(acc)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Vs All\n",
      "Initial_cost-  0.6931471805599391\n",
      "Final_cost-  0.00030601885892336946\n",
      "Initial_cost-  0.6931471805599391\n",
      "Final_cost-  0.00030601885892336946\n",
      "Initial_cost-  0.6931471805599391\n",
      "Final_cost-  0.00030601885892336946\n",
      "Initial_cost-  0.6931471805599391\n",
      "Final_cost-  0.023416731702795276\n",
      "Initial_cost-  0.6931471805599391\n",
      "Final_cost-  0.11954174787867652\n",
      "Initial_cost-  0.6931471805599391\n",
      "Final_cost-  0.513664579402055\n",
      "Initial_cost-  0.6931471805599391\n",
      "Final_cost-  0.9268563650318428\n",
      "Initial_cost-  0.6931471805599391\n",
      "Final_cost-  0.39966462684962967\n",
      "Initial_cost-  0.6931471805599391\n",
      "Final_cost-  0.12651762413367557\n",
      "Initial_cost-  0.6931471805599391\n",
      "Final_cost-  0.007264291499559555\n",
      "Initial_cost-  0.6931471805599391\n",
      "Final_cost-  0.00030601885892336946\n",
      "Accuracy- 0.4886621315192744\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
